The streamflow gauge, added as a ‘release measurement’ in the model, downstream of Tarbela Dam was used to calibrate the model. The calibration was based on the comparison of the actual releases, added in the ‘release measurement’ node, to the WEAP model releases. The calibration period from June 1976 to December 2001 was selected as a representative record of the entire 1976 to 2013 simulation period, considering mean annual flow and extreme events. During the period from June 1976 to December 2013, mean annual flow of the Indus was 82,526 cfs with a standard deviation of 9,546 cfs. While during the selected calibration period (June 1976-December 2001), mean annual flow was 83,096 cfs with a standard deviation of 10,091 cfs. Also, during the historical record (June 1976-December 2013), there were 14 floods and five drought events, making a total of 19 extreme events. During the selected calibration period (June 1976-December 2001), there were five floods and the same number of drought events. Therefore, the calibration period contained a sufficient number of extreme events. The model input parameters, provided in Appendix A, were of ‘verified’ quality, except evaporation, which was of ‘uncertain’ quality.  Therefore, the model was calibrated by changing the least certain input variable, which was evaporation. The average evaporation values for the calibration period (84.6 in) was representative of the historical record (86.9 in). The calibration was done by using an inbuilt WEAP calibration tool known as ‘PEST calibration’. In the PEST tool, the parameter to calibrate was selected as evaporation in the ‘add data variable’ user input form, and the range to modify this parameter was selected as 90% increase and a decrease of the base value (0.02 ft/day) with an increment of 10%. Starting with 90% decrease in the evaporation to 90% increase in the evaporation value, and with selecting ‘Auto Calculate’ for the result, 19 different reservoir release time series were computed downstream of the dam where ‘release measurement’ node was added in the model. The hydrograph of the 19 model releases was compared with the actual/observed releases, recorded at the same node in the model. To calculate the accuracy of the observed data and model data, the Nash-Sutcliffe Efficiency (NSE) Index was used:

                          NSE=1-  (∑_(t=1)^T(Qm-Qo)^2 /(∑_(t=1)^T(Qo-Qa)^2

where Qm, Qo, and Qa represent model discharge, observed discharge and mean of observed discharge, respectively. 

The NSE value close to 1 shows that the accuracy of the model is high (Nash and Sutcliffe 1970). The time series of the releases from the model with the increase in the evaporation by 30% (0.026 ft/day) resulted in the highest NSE index of 0.77. Therefore, 30% increase in the evaporation was selected as the chosen value for calibration. The timing of the model peaks matched well with the observed discharge values. However, the magnitude of the peak values is higher than the observed discharge values. Figure 3.3 shows the hydrograph of the observed releases with the modeled releases. 

The validation period was selected from January 2002 to December 2013. The validation results showed the NSE index value as 0.74. As observed with the calibration, the timing of the model peak matched with the timings of the observed discharge, but the model peak was higher than the observed peak values.
After developing, calibrating, and validating the model with the current operations under historical conditions, the next step was to explore the effect of altered reservoir operations on performance.
